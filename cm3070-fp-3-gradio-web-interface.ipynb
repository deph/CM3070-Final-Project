{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1fc8235-1c6b-4abe-95fc-115b7fec79fd",
   "metadata": {},
   "source": [
    "<div style='text-align: center;'>\n",
    "    <span style='font-size: 30px; font-weight: bold;'>\n",
    "        CM3070 Final Project\n",
    "    </span>\n",
    "</div>\n",
    "<div style='text-align: center;'>\n",
    "    <span style='font-size: 30px; font-weight: bold;'>\n",
    "        Final Report\n",
    "    </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34d3dc8-bc22-4419-991d-952089901f80",
   "metadata": {},
   "source": [
    "# 9. User Testing and Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1e86e75-4423-4a1a-9b5a-f898f77af731",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seirokusan/anaconda3/envs/tf_gardio_p39_pip_21mar/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-30 15:40:56.104808: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743320456.888523    5821 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743320457.075907    5821 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1743320458.752341    5821 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743320458.752377    5821 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743320458.752380    5821 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1743320458.752381    5821 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-30 15:40:58.887470: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "I0000 00:00:1743320572.101708    5821 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 7125 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at saved_models/model_bert_tf.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://a21897d156cc395e29.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://a21897d156cc395e29.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1743320627.133737    6229 service.cc:152] XLA service 0x763d28008ef0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1743320627.133806    6229 service.cc:160]   StreamExecutor device (0): NVIDIA GeForce GTX 1080, Compute Capability 6.1\n",
      "2025-03-30 15:43:47.524419: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1743320627.921854    6229 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1743320631.908567    6229 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------------------------\n",
    "# Libraries\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "import gradio as gr                                                      # For building the interface\n",
    "import numpy as np                                                       # For numerical operations\n",
    "import joblib                                                            # For model serialization\n",
    "import os                                                                # For interacting with the operating system\n",
    "import pandas as pd                                                      # For data manipulation\n",
    "from datetime import datetime                                            # For handling date and time\n",
    "import tensorflow as tf                                                  # For deep learning\n",
    "from tensorflow.keras.models import load_model                           # For loading Keras models\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences        # For padding sequences\n",
    "from transformers import TFBertForSequenceClassification, BertTokenizer  # For BERT-based models and tokenization\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "# Load Models\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "vectorizer_tfidf = joblib.load(\"saved_models/vectorizer_tfidf.pkl\")  # TF-IDF Vectorizer\n",
    "model_web_nb = joblib.load(\"saved_models/model_nb.pkl\")              # Naive Bayes\n",
    "model_web_lr = joblib.load(\"saved_models/model_lr_best.pkl\")         # Logistic Regression\n",
    "\n",
    "model_web_cnn = load_model(\"saved_models/model_cnn.keras\")           # CNN\n",
    "tokenizer_cnn = joblib.load(\"saved_models/tokenizer_cnn.pkl\")        # Tokenizer for CNN\n",
    "\n",
    "model_bert_tf = TFBertForSequenceClassification.from_pretrained(\"saved_models/model_bert_tf\")  # BERT\n",
    "tokenizer_bert_tf = BertTokenizer.from_pretrained(\"saved_models/model_bert_tf\")                # Tokenizer for BERT\n",
    "\n",
    "model_meta = joblib.load(\"saved_models/model_meta.pkl\")  # Stacking Model\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "# FUNCTION - Classifies the news article using multiple models and returns a final prediction\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "def classify_news(article_text):\n",
    "    meta_features = []\n",
    "    \n",
    "    # Naive Bayes\n",
    "    X_tfidf = vectorizer_tfidf.transform([article_text])\n",
    "    probs_nb = model_web_nb.predict_proba(X_tfidf)[0]  # (2,)\n",
    "    label_nb = \"Real\" if probs_nb[1] > probs_nb[0] else \"Fake\"\n",
    "    confidence_nb = probs_nb[1] if probs_nb[1] > probs_nb[0] else probs_nb[0]\n",
    "    meta_features.append(probs_nb[1])  # Store positive class probability\n",
    "\n",
    "    # Logistic Regression\n",
    "    probs_lr = model_web_lr.predict_proba(X_tfidf)[0]  # (2,)\n",
    "    label_lr = \"Real\" if probs_lr[1] > probs_lr[0] else \"Fake\"\n",
    "    confidence_lr = probs_lr[1] if probs_lr[1] > probs_lr[0] else probs_lr[0]\n",
    "    meta_features.append(probs_lr[1])\n",
    "\n",
    "    # CNN Model\n",
    "    seq = tokenizer_cnn.texts_to_sequences([article_text])\n",
    "    padded_seq = pad_sequences(seq, maxlen=500, padding='post', truncating='post')\n",
    "    probs_cnn = model_web_cnn.predict(padded_seq)[0]  # (1,)\n",
    "    probs_cnn = np.array([1 - probs_cnn[0], probs_cnn[0]])  # Convert to (2,)\n",
    "    label_cnn = \"Real\" if probs_cnn[1] > probs_cnn[0] else \"Fake\"\n",
    "    confidence_cnn = probs_cnn[1] if probs_cnn[1] > probs_cnn[0] else probs_cnn[0]\n",
    "    meta_features.append(probs_cnn[1])\n",
    "\n",
    "    # BERT Model\n",
    "    inputs = tokenizer_bert_tf(article_text, truncation=True, padding=True, max_length=128, return_tensors=\"tf\")\n",
    "    logits_bert_tf = model_bert_tf(**inputs).logits\n",
    "    probs_bert_tf = tf.nn.softmax(logits_bert_tf, axis=1).numpy()[0]  # (2,)\n",
    "    label_bert_tf = \"Real\" if probs_bert_tf[1] > probs_bert_tf[0] else \"Fake\"\n",
    "    confidence_bert_tf = probs_bert_tf[1] if probs_bert_tf[1] > probs_bert_tf[0] else probs_bert_tf[0]\n",
    "    meta_features.append(probs_bert_tf[1])\n",
    "\n",
    "    # Stacking Model (Final Prediction)\n",
    "    X_meta = np.array(meta_features).reshape(1, -1)  # Reshape for prediction\n",
    "    probs_meta = model_meta.predict_proba(X_meta)[0]  # (2,)\n",
    "    pred_class_meta = np.argmax(probs_meta)  # 0 = Fake, 1 = Real\n",
    "\n",
    "    label_meta = \"Fake\" if pred_class_meta == 0 else \"Real\"\n",
    "    confidence_meta = probs_meta[pred_class_meta]  # Extract confidence score\n",
    "\n",
    "    # Final Prediction: Combine the result and confidence\n",
    "    final_prediction = f\"{label_meta} (Confidence: {confidence_meta:.2f})\"\n",
    "    \n",
    "    # Model Predictions (for display)\n",
    "    model_predictions = \"\\n\".join([\n",
    "        f\"Naive Bayes: {label_nb} (Confidence: {confidence_nb:.2f})\",\n",
    "        f\"Logistic Regression: {label_lr} (Confidence: {confidence_lr:.2f})\",\n",
    "        f\"CNN: {label_cnn} (Confidence: {confidence_cnn:.2f})\",\n",
    "        f\"BERT: {label_bert_tf} (Confidence: {confidence_bert_tf:.2f})\"\n",
    "    ])\n",
    "    \n",
    "    return final_prediction, model_predictions\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "# FUNCTION - Save user feedback to a CSV file, including model predictions\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "def save_feedback(article_text, final_prediction, model_predictions, feedback_accuracy, feedback_confidence, \n",
    "                  feedback_performance, feedback_ui, feedback_usefulness, feedback_recommendation, feedback_text):\n",
    "\n",
    "    # Change '\\n' to ',' for saving to CSV\n",
    "    model_predictions_csv = model_predictions.replace(\"\\n\", \", \")\n",
    "    \n",
    "    # Collect feedback inputs\n",
    "    all_feedback = []\n",
    "\n",
    "    # Collect accuracy-related feedback (Radio: correct/wrong)\n",
    "    if feedback_accuracy:\n",
    "        all_feedback.append(feedback_accuracy)\n",
    "\n",
    "    # Collect confidence-related feedback (Checkbox: too high/low)\n",
    "    if feedback_confidence:\n",
    "        all_feedback.append(\"The confidence score seemed too high/low\")\n",
    "\n",
    "    # Collect performance-related feedback (Radio: fast/slow)\n",
    "    if feedback_performance:\n",
    "        all_feedback.append(feedback_performance)\n",
    "\n",
    "    # Collect UI-related feedback (Radio: easy/confusing)\n",
    "    if feedback_ui:\n",
    "        all_feedback.append(feedback_ui)\n",
    "\n",
    "    # Collect overall feedback (Radio: useful/not useful)\n",
    "    if feedback_usefulness:\n",
    "        all_feedback.append(feedback_usefulness)\n",
    "\n",
    "    # Collect recommendation feedback (Checkbox: recommend or not)\n",
    "    if feedback_recommendation:\n",
    "        all_feedback.append(\"I would recommend this to others\")\n",
    "\n",
    "    # Add additional textual feedback (open-ended)\n",
    "#    if feedback_text:\n",
    "#        all_feedback.append(feedback_text)\n",
    "    \n",
    "    # Prepare the feedback for saving (convert all feedback to string format)\n",
    "    all_feedback = [str(item) for item in all_feedback]\n",
    "\n",
    "    # Prepare data to be saved\n",
    "    data = {\n",
    "        \"Article Text\": article_text,                       # Store article text\n",
    "        \"Final Prediction\": final_prediction,               # Store final prediction (ensemble)\n",
    "        \"Model Predictions\": model_predictions_csv,  # Store individual model predictions\n",
    "        \"Feedback Selected\": \", \".join(all_feedback),       # Store all feedback options\n",
    "        \"User Feedback\": feedback_text                      # User's open-ended feedback\n",
    "    }\n",
    "\n",
    "    # Generate a timestamp for saving\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    FEEDBACK_FILE = f\"user_feedback/user_feedback_{timestamp}.csv\"\n",
    "\n",
    "    # Save feedback to CSV\n",
    "    df = pd.DataFrame([data])\n",
    "    if os.path.exists(FEEDBACK_FILE):\n",
    "        df.to_csv(FEEDBACK_FILE, mode='a', header=False, index=False)  # Append to existing file\n",
    "    else:\n",
    "        df.to_csv(FEEDBACK_FILE, mode='w', header=True, index=False)  # Create new file\n",
    "\n",
    "    return \"✅ Feedback saved! Thank you for your input.\"\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "# Create Gradio Interface\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "with gr.Blocks() as iface:\n",
    "    gr.Markdown(\"# 📰 Fake News Detector (AI-Powered)\")\n",
    "\n",
    "    with gr.Row():\n",
    "        with gr.Column(scale=3):  # Input section (60%)\n",
    "            article_text = gr.Textbox(\n",
    "                lines=6,\n",
    "                placeholder=\"Paste a news article here...\",\n",
    "                label=\"📝 Enter News Article\"\n",
    "            )\n",
    "            submit_button = gr.Button(\"Classify News\", elem_id=\"submit_button\")  # Ensure ID for the button\n",
    "\n",
    "        with gr.Column(scale=2):  # Output section (40%)\n",
    "            prediction_output = gr.Textbox(label=\"📢 Final Prediction (Ensemble)\", interactive=False)\n",
    "            model_predictions_output = gr.Textbox(lines=4, label=\"📊 Model Predictions\", interactive=False)\n",
    "\n",
    "    submit_button.click(fn=classify_news, inputs=[article_text], outputs=[prediction_output, model_predictions_output])\n",
    "\n",
    "    # User Feedback Section\n",
    "    gr.Markdown(\"## 📝 User Feedback\")\n",
    "\n",
    "    # Section 1 - Prediction Accuracy and Confidence Level\n",
    "    gr.Markdown(\"### 🎯 Prediction Accuracy Issues\")\n",
    "    feedback_accuracy = gr.Radio(\n",
    "        choices=[\n",
    "            \"The prediction was correct\",\n",
    "            \"The prediction was wrong\",\n",
    "        ],\n",
    "        label=\"Select any that apply\",\n",
    "        elem_id=\"feedback_accuracy\"\n",
    "    )\n",
    "\n",
    "    feedback_confidence = gr.Checkbox(\n",
    "        label=\"The confidence score seemed too high/low\",\n",
    "        elem_id=\"feedback_confidence\"\n",
    "    )\n",
    "    \n",
    "    # Section 2 - Model Performance & Speed\n",
    "    gr.Markdown(\"### ⚡ Model Performance & Speed\")\n",
    "    feedback_performance = gr.Radio(\n",
    "        choices=[\n",
    "            \"The response was fast\",\n",
    "            \"The response was slow\",\n",
    "        ],\n",
    "        label=\"Select any that apply\",\n",
    "        elem_id=\"feedback_performance\"\n",
    "    )\n",
    "    \n",
    "    # Section 3 - User Experience and Interface\n",
    "    gr.Markdown(\"### 🎨 User Experience & Interface\")\n",
    "    feedback_ui = gr.Radio(\n",
    "        choices=[\n",
    "            \"The interface was easy to use\",\n",
    "            \"The interface was confusing\",\n",
    "        ],\n",
    "        label=\"Select any that apply\",\n",
    "        elem_id=\"feedback_ui\"\n",
    "    )\n",
    "\n",
    "    # Section 4 - Overall Feedback\n",
    "    gr.Markdown(\"### 🌟 Overall Feedback\")\n",
    "    feedback_usefulness = gr.Radio(\n",
    "        choices=[\n",
    "            \"The system was useful\",\n",
    "            \"The system was not useful\"\n",
    "        ],\n",
    "        label=\"Select any that apply\",\n",
    "        elem_id=\"feedback_usefulness\"\n",
    "    )\n",
    "\n",
    "    feedback_recommendation = gr.Checkbox(\n",
    "        label=\"I would recommend this to others\",\n",
    "        elem_id=\"feedback_recommendation\"\n",
    "    )\n",
    "    \n",
    "    feedback_text = gr.Textbox(lines=2, placeholder=\"Any additional feedback?\", label=\"Your Feedback\", elem_id=\"feedback_text\")\n",
    "\n",
    "    # Submit feedback button\n",
    "    save_feedback_button = gr.Button(\"Submit Feedback\", elem_id=\"save_feedback_button\")\n",
    "    feedback_message = gr.Textbox(label=\"📌 Feedback Status\", interactive=False)\n",
    "\n",
    "    save_feedback_button.click(\n",
    "        fn=save_feedback,\n",
    "        inputs=[\n",
    "            article_text, prediction_output, model_predictions_output,\n",
    "            feedback_accuracy, feedback_confidence, feedback_performance, feedback_ui, \n",
    "            feedback_usefulness, feedback_recommendation, feedback_text\n",
    "        ],\n",
    "        outputs=[feedback_message]\n",
    "    )\n",
    "\n",
    "iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f83d158-143a-482c-8e99-b0cac818b06b",
   "metadata": {},
   "source": [
    "# 10. Retrieving user feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a92b77ba-5ddf-4571-a380-e4f1a5d537c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "Running on public URL: https://82c9b3f078664888d6.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://82c9b3f078664888d6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --------------------------------------------------------------------------------------------------\n",
    "# Libraries\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "import gradio as gr            # For building the interface\n",
    "import os                      # For interacting with the operating system\n",
    "import pandas as pd            # For data manipulation\n",
    "from datetime import datetime  # For handling date and time\n",
    "\n",
    "# Define the folder where feedback files are stored\n",
    "FEEDBACK_FOLDER = \"user_feedback\"\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "# FUNCTION - Get all feedback files sorted by oldest first\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "def get_sorted_feedback_files():\n",
    "    feedback_path = os.path.join(os.getcwd(), FEEDBACK_FOLDER)  # Full path\n",
    "    if not os.path.exists(feedback_path):  \n",
    "        return []\n",
    "    \n",
    "    files = [os.path.join(feedback_path, f) for f in os.listdir(feedback_path) \n",
    "             if f.startswith(\"user_feedback_\") and f.endswith(\".csv\")]\n",
    "\n",
    "    # ✅ Sort files by actual datetime extracted from filename\n",
    "    def extract_datetime(file_path):\n",
    "        try:\n",
    "            filename = os.path.basename(file_path)\n",
    "            parts = filename.split(\"_\")\n",
    "            date_part = parts[2]                # YYYYMMDD\n",
    "            time_part = parts[3].split(\".\")[0]  # HHMMSS\n",
    "            \n",
    "            return datetime.strptime(f\"{date_part} {time_part}\", \"%Y%m%d %H%M%S\")\n",
    "        except Exception as e:\n",
    "            return datetime.min  # If error, place at the very start\n",
    "\n",
    "    files.sort(key=extract_datetime, reverse=True)  # ✅ Sorted by datetime (newest → oldest)\n",
    "\n",
    "    return files\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "# FUNCTION - Extract timestamp from filename\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "def extract_timestamp(filename):\n",
    "    try:\n",
    "        parts = filename.split(\"_\")         # Split filename by underscores\n",
    "        date_part = parts[2]                # Extract YYYYMMDD\n",
    "        time_part = parts[3].split(\".\")[0]  # Extract HHMMSS without \".csv\"\n",
    "\n",
    "        formatted_date = f\"{date_part[:4]}-{date_part[4:6]}-{date_part[6:]}\"  # YYYY-MM-DD\n",
    "        formatted_time = f\"{time_part[:2]}:{time_part[2:4]}:{time_part[4:]}\"  # HH:MM:SS\n",
    "\n",
    "        return f\"{formatted_date} {formatted_time}\"\n",
    "    except Exception as e:\n",
    "        return \"Unknown Date & Time\"  # Handle unexpected filename formats\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "# FUNCTION - Load and format feedback data as a Markdown table\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "def load_all_feedback():\n",
    "    feedback_files = get_sorted_feedback_files()\n",
    "\n",
    "    if not feedback_files:\n",
    "        return \"No feedback data found.\"\n",
    "\n",
    "    feedback_text = \"\"\n",
    "    total_feedback = 0  # Initialize total feedback counter\n",
    "    feedback_counts = {\n",
    "        \"The prediction was correct\": 0,\n",
    "        \"The prediction was wrong\": 0,\n",
    "        \"The confidence score seemed too high/low\": 0,\n",
    "        \"The response was fast\": 0,\n",
    "        \"The response was slow\": 0,\n",
    "        \"The interface was easy to use\": 0,\n",
    "        \"The interface was confusing\": 0,\n",
    "        \"The system was useful\": 0,\n",
    "        \"The system was not useful\": 0,\n",
    "        \"I would recommend this to others\": 0\n",
    "    }\n",
    "\n",
    "    # Read all CSV files one by one (now sorted newest → oldest)\n",
    "    entry_count = 1\n",
    "    for file in feedback_files:\n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            filename = os.path.basename(file)        # Get filename only\n",
    "            timestamp = extract_timestamp(filename)  # Extract date & time from filename\n",
    "\n",
    "            for index, row in df.iterrows():\n",
    "                feedback_items = row[\"Feedback Selected\"].replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\").split(\", \")\n",
    "\n",
    "                # Update feedback counts\n",
    "                for feedback_item in feedback_items:\n",
    "                    if feedback_item in feedback_counts:\n",
    "                        feedback_counts[feedback_item] += 1\n",
    "\n",
    "                model_predictions = row[\"Model Predictions\"].replace(\"[\", \"\").replace(\"]\", \"\").replace(\"'\", \"\").split(\", \")\n",
    "                \n",
    "                feedback_text += f\"\\n### Feedback Entry {entry_count}\\n\\n\"\n",
    "                feedback_text += \"| **Field** | **Details** |\\n\"\n",
    "                feedback_text += \"|---|---|\\n\"\n",
    "                feedback_text += f\"| 📂 Filename | {filename} |\\n\"\n",
    "                feedback_text += f\"| 🕒 Date & Time | {timestamp} |\\n\"\n",
    "                feedback_text += f\"| 📰 Article | {row['Article Text']} |\\n\"\n",
    "                feedback_text += f\"| 📢  Final Prediction (Ensemble) | {row['Final Prediction']} |\\n\"\n",
    "\n",
    "                feedback_text += \"| 📊 Model Predictions | \"\n",
    "                feedback_text += \"<br>\".join(model_predictions) + \" |\\n\"\n",
    "\n",
    "                feedback_text += \"| 📝 User Feedback | \"\n",
    "                feedback_text += \"<br>\".join(feedback_items) + \" |\\n\"\n",
    "\n",
    "                user_feedback = row['User Feedback'].replace('\\n', '<br>')\n",
    "                feedback_text += f\"| ✏ Additional Comments | {user_feedback} |\\n\"\n",
    "\n",
    "                feedback_text += \"\\n---\\n\" \n",
    "                entry_count += 1             # Increment feedback entry count\n",
    "                total_feedback += 1          # Count total feedback entries\n",
    "\n",
    "        except Exception as e:\n",
    "            feedback_text += f\"\\n⚠ Error reading {file}: {e}\\n\"\n",
    "\n",
    "    # Add total feedback count and consolidated feedback counts table at the top\n",
    "    feedback_summary = f\"## 📊 Total Feedback Entries: {total_feedback}\\n\\n\"\n",
    "\n",
    "    # Create a table showing counts of each feedback item\n",
    "    feedback_summary += \"### Consolidated Feedback Counts\\n\\n\"\n",
    "    feedback_summary += \"| **Feedback Type** | **Count** |\\n\"\n",
    "    feedback_summary += \"|---|---|\\n\"\n",
    "    \n",
    "    for feedback_type, count in feedback_counts.items():\n",
    "        feedback_summary += f\"| {feedback_type} | {count} |\\n\"\n",
    "\n",
    "    # Return consolidated feedback summary and detailed feedback entries\n",
    "    return feedback_summary + feedback_text.strip()\n",
    "\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "# Automatically refresh feedback when new feedback is saved\n",
    "# --------------------------------------------------------------------------------------------------\n",
    "with gr.Blocks() as iface:\n",
    "    gr.Markdown(\"# 📝 All User Feedback Records (Newest First)\")\n",
    "\n",
    "    # Display existing feedback\n",
    "    feedback_display = gr.Markdown(load_all_feedback())  \n",
    "\n",
    "    # Button to manually refresh\n",
    "    refresh_button = gr.Button(\"Refresh Feedback\")  \n",
    "\n",
    "    # Refresh feedback when clicked\n",
    "    refresh_button.click(fn=load_all_feedback, inputs=[], outputs=[feedback_display])  \n",
    "\n",
    "iface.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4e5756-c610-4180-adba-f7706102c7c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_gradio_kernel",
   "language": "python",
   "name": "tf_gardio_p39_pip_21mar"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
